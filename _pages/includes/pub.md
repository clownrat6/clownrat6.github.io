
# üìù Publications 

## üéûÔ∏è Multi-modal LLM (Video Understanding)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Qwen2.5-VL</div><img src='images/qwen2.5vl_pipeline.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Qwen2.5-VL Technical Report](https://arxiv.org/pdf/2502.13923) <br>
**Core Contributors**: Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, ..., **Zesen Cheng**,
Hang Zhang, Zhibo Yang, Haiyang Xu, Junyang Lin

[**Project**](https://qwenlm.github.io/blog/qwen2.5-vl/) \| 
[**Code**](https://github.com/QwenLM/Qwen2.5-VL) [![](https://img.shields.io/github/stars/QwenLM/Qwen2.5-VL?style=social&label=Qwen2.5-VL+Stars)](https://github.com/QwenLM/Qwen2.5-VL) \| 
[![](https://img.shields.io/badge/Demo-Hugging_Face-CFAFD4)](https://huggingface.co/spaces/Qwen/Qwen2.5-VL-72B-Instruct) 
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:IWHjjKOFINEC'></span></strong>

<!-- - FastSpeech is the first fully parallel end-to-end speech synthesis model.
- **Academic Impact**: This work is included by many famous speech synthesis open-source projects, such as [ESPNet ![](https://img.shields.io/github/stars/espnet/espnet?style=social)](https://github.com/espnet/espnet). Our work are promoted by more than 20 media and forums, such as [Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/UkFadiUBy-Ymn-zhJ95JcQ)„ÄÅ[InfoQ](https://www.infoq.cn/article/tvy7hnin8bjvlm6g0myu).
- **Industry Impact**: FastSpeech has been deployed in [Microsoft Azure TTS service](https://techcommunity.microsoft.com/t5/azure-ai/neural-text-to-speech-extends-support-to-15-more-languages-with/ba-p/1505911) and supports 49 more languages with state-of-the-art AI quality. It was also shown as a text-to-speech system acceleration example in [NVIDIA GTC2020](https://resources.nvidia.com/events/GTC2020s21420). -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VideoLLaMA3</div><img src='images/videollama3_pipeline.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding](https://arxiv.org/pdf/2501.13106) <br>
Boqiang Zhang\* Kehan Li\*, **Zesen Cheng**\*, Zhiqiang Hu\*, Yuqian Yuan\*, Guanzheng Chen\*, Sicong Leng\*, Yuming Jiang\*, Hang Zhang\*, Xin Li\*, Peng Jin, Wenqi Zhang, Fan Wang, Lidong Bing, Deli Zhao

[**Code**](https://github.com/DAMO-NLP-SG/VideoLLaMA3) [![](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA3?style=social&label=VideoLLaMA3+Stars)](https://github.com/DAMO-NLP-SG/VideoLLaMA3) \|
[![hf_space](https://img.shields.io/badge/ü§ó-Image_Demo-9C276A.svg)](https://huggingface.co/spaces/lixin4ever/VideoLLaMA3-Image) \|
[![hf_space](https://img.shields.io/badge/ü§ó-Video_Demo-9C276A.svg)](https://huggingface.co/spaces/lixin4ever/VideoLLaMA3) \|
[![hf_paper](https://img.shields.io/badge/ü§ó-Paper%20In%20HF-red.svg)](https://huggingface.co/papers/2501.13106)
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:ZeXyd9-uunAC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">VideoLLaMA2</div><video 
controls
autoplay
muted
loop
playsinline
style="width: 100%; max-width: 600px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.2);"
src="https://github.com/DAMO-NLP-SG/VideoLLaMA2/assets/18526640/e0e7951c-f392-42ed-afad-b2c7984d3e38" type="video/webm"></video></div></div>
<div class='paper-box-text' markdown="1">

[VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs](https://arxiv.org/pdf/2502.13923) <br>
**Zesen Cheng**\*, **Sicong Leng**\*, **Hang Zhang**\*, **Yifei Xin**\*, **Xin Li**\*, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, Lidong Bing

[**Code**](https://github.com/DAMO-NLP-SG/VideoLLaMA2) [![](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoLLaMA2?style=social&label=VideoLLaMA2+Stars)](https://github.com/DAMO-NLP-SG/VideoLLaMA2) \|
[![hf_space](https://img.shields.io/badge/ü§ó-AV--Demo-9C276A.svg)](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2-AV) \|
[![hf_space](https://img.shields.io/badge/ü§ó-Demo-9C276A.svg)](https://huggingface.co/spaces/lixin4ever/VideoLLaMA2) \|
[![hf_paper](https://img.shields.io/badge/ü§ó-Paper%20In%20HF-red.svg)](https://huggingface.co/papers/2406.07476)
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:Wp0gIr-vW9MC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CMM</div><img src='images/data_comp1.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio](https://arxiv.org/pdf/2410.12787) <br>
Sicong Leng\*, Yun Xing\*, **Zesen Cheng**\*, Yang Zhou, Hang Zhang, Xin Li, Deli Zhao, Shijian Lu, Chunyan Miao, Lidong Bing

[**Project**](https://cmm-damovl.site/) 
[**Code**](https://github.com/DAMO-NLP-SG/CMM) [![](https://img.shields.io/github/stars/DAMO-NLP-SG/CMM?style=social&label=CMM+Stars)](https://github.com/DAMO-NLP-SG/CMM) \|
[![hf_data](https://img.shields.io/badge/ü§ó-CMM-9C276A.svg)](https://huggingface.co/datasets/DAMO-NLP-SG/CMM)
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:YOwf2qJgpHMC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025 Highlight</div><img src='https://github.com/user-attachments/assets/2c19838b-43d8-4145-b28c-903f3d76f8ab' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy](https://openaccess.thecvf.com/content/CVPR2025/papers/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.pdf) **<font color="red">(Hightlight)</font>** <br>
**Zesen Cheng**\*, Hang Zhang\*, Kehan Li\*, Sicong Leng, Zhiqiang Hu, Fei Wu, Deli Zhao, Xin Li, Lidong Bing

[**Code**](https://github.com/DAMO-NLP-SG/Inf-CLIP) \| 
[![hf_paper](https://img.shields.io/badge/ü§ó-Paper%20In%20HF-red.svg)](https://huggingface.co/papers/2410.17243) \| 
[![PyPI](https://img.shields.io/badge/PyPI-Inf--CL-9C276A.svg)](https://pypi.org/project/inf-cl) 
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:r0BpntZqJG4C'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/videorefer/demo.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM](https://arxiv.org/pdf/2501.00599) <br>
Yuqian Yuan, Hang Zhang, Wentong Li, **Zesen Cheng**, et al.

[**Project**](https://damo-nlp-sg.github.io/VideoRefer/) \| [**Code**](https://github.com/DAMO-NLP-SG/VideoRefer) [![](https://img.shields.io/github/stars/DAMO-NLP-SG/VideoRefer?style=social&label=VideoRefer+Stars)](https://github.com/DAMO-NLP-SG/VideoRefer) \| [![](https://img.shields.io/badge/Benchmark-Hugging_Face-96D03A)](https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-Bench) \| [![](https://img.shields.io/badge/Dataset-Hugging_Face-E59FB6)](https://huggingface.co/datasets/DAMO-NLP-SG/VideoRefer-700K) \| [![](https://img.shields.io/badge/Demo-Hugging_Face-CFAFD4)](https://huggingface.co/spaces/lixin4ever/VideoRefer-VideoLLaMA3) <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:L8Ckcad2t8MC'></span></strong>

</div>
</div>

## üß© Image/Video Segmentation

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/tar/tar_pipeline.png' alt="tar" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Temporal-aware Query Routing for Real-time Video Instance Segmentation]() <br> **Zesen Cheng**, Kehan Li, Yian Zhao, et al. 
<!-- <span style="color:red">(Spotlight)</span> [**Project**](https://real3dportrait.github.io/)  -->
<!-- | [**Code**](https://github.com/yerfor/Real3DPortrait) -->
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025</div><img src='images/brivis/BriVIS_pipeline.png' alt="tar" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Aligning Instance Brownian Bridge with Texts for Open-vocabulary Video Instance Segmentation](https://arxiv.org/pdf/2401.09732) <br> **Zesen Cheng**, Kehan Li, Li Hao, Peng Jin, et al. 

[**Code**](https://github.com/clownrat6/OpenVIS) <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:5nxA0vEk-isC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/pvd/pvd_pipeline.png' alt="tar" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Parallel Vertex Diffusion for Unified Visual Grounding](https://ojs.aaai.org/index.php/AAAI/article/view/27896/27815) <br> **Zesen Cheng**, Kehan Li, Peng Jin, et al. 

<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:zYLM7Y9cAGgC'></span></strong>
</div>
</div>

- ``CVPR 2024`` **<font color="red">(Hightlight)</font>** [GraCo: Granularity-Controllable Interactive Segmentation](https://openaccess.thecvf.com/content/CVPR2024/papers/Zhao_GraCo_Granularity-Controllable_Interactive_Segmentation_CVPR_2024_paper.pdf) <br> Yian Zhao, Kehan Li, **Zesen Cheng**, Pengchong Qiao, Xiawu Zheng, et al. \| [![](https://img.shields.io/github/stars/Zhao-Yian/GraCo?style=social&label=Code+Stars)](https://github.com/Zhao-Yian/GraCo) <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:8k81kl-MbHgC'></span></strong>

- ``Neurocomputing 2024`` [Hierarchical collaboration for referring image segmentation](https://www.sciencedirect.com/science/article/abs/pii/S0925231224014036) <br> Wei Zhang, **Zesen Cheng**, et al.

- ``ICCV 2023`` [Multi-granularity Interaction Simulation for Unsupervised Interactive Segmentation](http://openaccess.thecvf.com/content/ICCV2023/papers/Li_Multi-granularity_Interaction_Simulation_for_Unsupervised_Interactive_Segmentation_ICCV_2023_paper.pdf) <br> 
Kehan Li, Yian Zhao, Zhennan Wang, **Zesen Cheng**, Peng Jin, et al. <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:Tyk-4Ss8FVUC'></span></strong>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IJCAI 2023</div><img src='images/wico/wico_pipeline.png' alt="tar" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[WiCo: Win-win Cooperation of Bottom-up and Top-down Referring Image Segmentation](https://arxiv.org/pdf/2306.10750) <br> **Zesen Cheng**, Peng Jin, Hao Li, Kehan Li, et al. 

<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:LkGwnXOMwfcC'></span></strong>
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/ocr/ocr_pipeline.png' alt="tar" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Out-of-Candidate Rectification for Weakly-supervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Cheng_Out-of-Candidate_Rectification_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2023_paper.pdf) <br>
**Zesen Cheng**, Pengchong Qiao, Kehan Li, Siheng Li, et al. 

<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:W7OEmFMy1HYC'></span></strong>

<!-- <span style="color:red">(Spotlight)</span> [**Project**](https://real3dportrait.github.io/)  -->
<!-- | [**Code**](https://github.com/yerfor/Real3DPortrait) -->
</div>
</div>

- ``CVPR 2023`` **<font color="red">(Hightlight)</font>** [ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Li_ACSeg_Adaptive_Conceptualization_for_Unsupervised_Semantic_Segmentation_CVPR_2023_paper.pdf) <br> Kehan Li, Zhennan Wang, **Zesen Cheng**, Runyi Yu, et al. <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:eQOLeE2rZwMC'></span></strong>

- ``CVPR 2023`` [EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding](https://openaccess.thecvf.com/content/CVPR2023/papers/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.pdf) <br> Yanmin Wu, Xinhua Cheng, Renrui Zhang, **Zesen Cheng**, Jian Zhang. \| [![](https://img.shields.io/github/stars/yanmin-wu/EDA?style=social&label=Code+Stars)](https://github.com/yanmin-wu/EDA) <strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:YsMSGLbcyi4C'></span></strong>

- ``Medical Physics 2021`` [Integrating multiple MRI sequences for pelvic organs segmentation via the attention mechanism](https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.15285) <br> Sijuan Huang\*, **Zesen Cheng**\*, et al. 
<strong><span class='show_paper_citations' data='Jkkp8JAAAAAJ:IjCSPb-OGe4C'></span></strong>

## Others

- ``ECCV 2024`` [Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation](https://arxiv.org/pdf/2407.10528) <br> Peng Jin, Hao Li, **Zesen Cheng**, Kehan Li, Runyi Yu, et al.
- ``ECCV 2024`` [FreestyleRet: Retrieving Images from Style-Diversified Queries](https://arxiv.org/pdf/2312.02428) <br> Hao Li, Curise Jia, Peng Jin, **Zesen Cheng**, Kehan Li, et al. \| [![](https://img.shields.io/github/stars/CuriseJia/FreeStyleRet?style=social&label=Code+Stars)](https://github.com/CuriseJia/FreeStyleRet)
- ``PRCV 2023`` **<font color="red">(Oral)</font>** [Object-Aware Transfer-Based Black-Box Adversarial Attack on Object Detector](https://www.researchgate.net/profile/Zesen-Cheng/publication/376888270_Object-Aware_Transfer-Based_Black-Box_Adversarial_Attack_on_Object_Detector/links/67c5c746645ef274a49a1ea7/Object-Aware-Transfer-Based-Black-Box-Adversarial-Attack-on-Object-Detector.pdf) <br> Zhuo Leng, **Zesen Cheng**, et al.
- ``ICCV 2023`` [DiffusionRet: Generative Text-Video Retrieval with Diffusion Model](https://openaccess.thecvf.com/content/ICCV2023/papers/Jin_DiffusionRet_Generative_Text-Video_Retrieval_with_Diffusion_Model_ICCV_2023_paper.pdf) <br> Peng Jin, Hao Li, **Zesen Cheng**, Kehan Li, et al. \| [![](https://img.shields.io/github/stars/jpthu17/DiffusionRet?style=social&label=Code+Stars)](https://github.com/jpthu17/DiffusionRet)
- ``IJCAI 2023`` [Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment](https://arxiv.org/pdf/2305.12218) <br> Peng Jin, Hao Li, **Zesen Cheng**, Jinfa Huang, et al.
- ``IJCAI 2023`` [TG-VQA: Ternary Game of Video Question Answering](https://arxiv.org/pdf/2305.10049) <br> Hao Li, Peng Jin, **Zesen Cheng**, et al. 
